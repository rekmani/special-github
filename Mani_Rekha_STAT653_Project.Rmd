---
title: "STAT 653 Project"
author: "Rekha Mani"
date: "5/4/2019"
output:
  word_document: default
  html_notebook: default
---


```{r warning=FALSE, message=FALSE}
# Includes all the library
library(tidytext)  # text mining
#install.packages("tidyverse")
library(tidyverse) # data manipulation
#install.packages("quanteda")
library(quanteda) # data cleaning
#install.packages("textclean")
library(textclean)
library(ggplot2)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("wordcloud2")
library(wordcloud2)
#install.packages("igraph") 
#install.packages("ggraph")
library(igraph)
library(ggraph)
#install.packages("widyr")
library(widyr)
#install.packages("tm")
library(tm)
#install.packages("topicmodels")
library(topicmodels)
#install.packages("plotly") # interactive plot
#library(plotly)
#install.packages("ggrepel")
library(ggrepel)

```


# I Text Mining and Exploratory Analysis

```{r}
## I Text Mining and Exploratory Analysis

# import dataset
song_lyrics<-read_csv("C:/Rekha/Personal/NLP/billboard_lyrics_1964-2015.csv")

dim(song_lyrics)

colnames(song_lyrics)

# replace contractions
fix_contractions<-function(doc)
{
  doc<-gsub("aint", "are not", doc)
  doc<-gsub("dont", "do not", doc)
  doc<-gsub("youve","you have",doc)
  doc<-gsub("wont","will not",doc)
  doc<-gsub("wanna","want to", doc)
  doc<-gsub("gonna","going to", doc)
  doc<-gsub("wouldnt","would not", doc)
  doc<-gsub("shouldnt","should not",doc)
  doc<-gsub("thats","that is", doc)
  doc<-gsub("hows","how is", doc)
  doc<-gsub("youre","you are",doc)
  doc<-gsub("youll","you will",doc)
  return(doc)
}

song_lyrics$Lyrics<-sapply(song_lyrics$Lyrics,fix_contractions)

# remove special char
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9]", " ", x)
song_lyrics$Lyrics<-sapply(song_lyrics$Lyrics,removeSpecialChars)

# remove numbers
removenumbers<-function(num) gsub("[0-9]","",num)
song_lyrics$Lyrics<-sapply(song_lyrics$Lyrics,removenumbers)

# remove underscore
remove_underscore<-function(text) gsub("[_]"," ",text)
song_lyrics$Lyrics<-sapply(song_lyrics$Lyrics,remove_underscore)


# check if underscore is removed
song_lyrics$Lyrics[5096]

# check if contractions are replaced and numbers removed
song_lyrics$Lyrics[348]

# notice missing lyrics
song_lyrics$Lyrics[463]  

# notice repeated and nonsensical words like do da do da da da
song_lyrics$Lyrics[1817] 
song_lyrics$Lyrics[2744]  

# notice punctuations
song_lyrics$Lyrics[869] 
song_lyrics$Lyrics[1928] 


# removing numbers, punctuations, symbols and hyphens
lyrics.tokens <- tokens(song_lyrics$Lyrics, what = "word", remove_numbers=TRUE,remove_punct=TRUE, remove_symbols=TRUE, remove_hyphens=TRUE)

# remove stopwords
lyrics.tokens<-tokens_select(lyrics.tokens, stopwords(), selection = "remove")

# stemming
lyrics.tokens<-tokens_wordstem(lyrics.tokens, language = "english")


# Convert to DFM
lyrics.tokens.dfm<-dfm(lyrics.tokens)

lyrics.tokens.dfm

# transform to matrix
lyrics.tokens.matrix<-as.matrix(lyrics.tokens.dfm)

#View(lyrics.tokens.matrix)
dim(lyrics.tokens.matrix) 

lyrics.song<-convert(lyrics.tokens.dfm, to="data.frame")


songs_df<-cbind(Rank=song_lyrics$Rank, Song=song_lyrics$Song,Artist=song_lyrics$Artist,Year=song_lyrics$Year,lyrics.song)



# Create new field  called decade to group the years to view song trends over the years

song_lyrics<- song_lyrics %>% mutate(decade=
                                 ifelse(Year %in% 1965:1969, "1960s",
                                 ifelse(Year %in% 1970:1979,"1970s",
                                 ifelse(Year %in% 1980:1989,"1980s",
                                ifelse(Year %in% 1990:1999,"1990s",
                                ifelse(Year %in% 2000:2009, "2000s",
                               ifelse(Year %in% 2010:2015,"2010s",
                               "NA")))))))

dim(song_lyrics)

# Tokenize
song_lyrics_token<-song_lyrics %>% unnest_tokens(word,Lyrics)
song_lyrics_filtered<-song_lyrics_token %>% anti_join(stop_words) %>% distinct() %>% filter(nchar(word)>3)

# count most common words
song_lyrics_filtered %>% count(word,sort = TRUE) 

# visualize most common words
song_lyrics_filtered %>% count(word,sort = TRUE) %>% filter(n>500) %>% mutate(word=reorder(word,n)) %>% ggplot(aes(word,n)) + geom_col() +
  labs(title="Most common words in Billboard songs from 1965 - 2015") + xlab(NULL) + ylab(NULL) +  coord_flip()

# Love seems to be most common topic. 

# visualize most common words using wordcloud
song_lyrics_count<-song_lyrics_filtered %>% count(word,sort = TRUE)
wordcloud2(song_lyrics_count[1:300, ], size = .5)

# so far we have seen top words across all the songs. Lets see trending words
trending_words<-song_lyrics_filtered %>% group_by(decade) %>% count(word,decade,sort=TRUE) %>% slice(seq_len(8)) %>% ungroup() %>% arrange(decade,n) %>% mutate(row=row_number())
trending_words

trending_words %>% ggplot(aes(row,n,fill=decade)) + geom_col(show.legend = NULL) + facet_wrap(~decade,scales = "free") + coord_flip() + scale_x_continuous(breaks=trending_words$row,labels = trending_words$word)

# Clearly love, time, and baby are trending words.

# The method we have seen so far looks at the entire dataset but it has not addressed how to quantify just how important various terms are in the document with respect to the entire collection. we have looked at term frequency and removed stop words but this is not sophisticated approach. Lets use TF-IDF approach to examine the words over the time

tfidf_words_decade<-song_lyrics %>% unnest_tokens(word,Lyrics) %>% distinct() %>% filter(nchar(word)>3) %>% count(word,decade, sort=TRUE) %>% ungroup() %>% bind_tf_idf(word,decade,n) %>% arrange(desc(tf_idf))
tfidf_words_decade

# top 8 words for each year
top_tfidf_words_decade<-tfidf_words_decade %>% group_by(decade) %>% slice(seq_len(8)) %>% ungroup() %>% arrange(decade,tf_idf) %>% mutate(row=row_number())
top_tfidf_words_decade

# visualize TF-IDF
top_tfidf_words_decade %>% ggplot(aes(row,tf_idf,fill=decade)) + geom_col(show.legend = NULL) + labs(x=NULL , y = "TF-IDF" , title = "Important words using TF-IDF over years") + facet_wrap(~decade,ncol=3, nrow=2, scales = "free") + scale_x_continuous(breaks=top_tfidf_words_decade$row,labels =  top_tfidf_words_decade$word) + coord_flip()

# Notice the trending words like love, time, baby, girl are no longer prevalent. This shows complete different set of words which gives us some idea about other common words used in songs over the years. Wow! you can see that we are getting into deeper level of insight now. 
  
```


# II Sentiment Analysis


```{r}
# First we will explore sentiment lexicons and word count

new_sentiments<-sentiments %>% filter(lexicon != "loughran") %>% mutate(sentiment=ifelse(lexicon == "AFINN" & score > 0, "positive", ifelse(lexicon == "AFINN" & score < 0, "Negative", sentiment))) %>% group_by(lexicon) %>% mutate(words_in_lexicon=n_distinct(word)) %>% ungroup()

new_sentiments %>% group_by(lexicon, sentiment, words_in_lexicon) %>% summarise(distinct_words=n_distinct(word)) %>% ungroup() %>% spread(sentiment, distinct_words)

# This table givs us an idea of size and structure of each lexicon
# In order to determine which lexicon is more applicable to the lyrics, lets take a look at the match ratio of words that are common to both lexicon and lyrics
#colnames(new_sentiments)
#colnames(song_lyrics_filtered)


song_lyrics_filtered %>% mutate(distinct_words_in_lyrics=n_distinct(word)) %>% inner_join(new_sentiments) %>% group_by(lexicon, distinct_words_in_lyrics,words_in_lexicon) %>% summarize(lex_match_words=n_distinct(word)) %>% ungroup() %>% mutate(match_ratio=lex_match_words/distinct_words_in_lyrics) %>% select(lexicon,lex_match_words,distinct_words_in_lyrics,match_ratio)

# NRC lexicon has more distinct words from the lyrics when compared to bing and AFINN. Also, notice the match ratio is very low. No lexicon could have all the words nor should they. Many words are considered as neutral and would not have an associated sentiment. 

# Now that we have fundamental understanding of lexicon, lets apply this knowledge to do sentiment analysis

lyrics_bing<-song_lyrics_filtered %>% inner_join(get_sentiments("bing"))
lyrics_nrc<-song_lyrics_filtered %>% inner_join(get_sentiments("nrc"))

#Since words can appear in multiple categories in NRC, such as Negative/Fear or Positive/Joy, you'll also create a subset without the positive and negative categories to use later on.
lyrics_nrc_sub<-song_lyrics_filtered %>% inner_join(get_sentiments("nrc")) %>% filter(!sentiment %in% c('positive','negative'))

# lets start by graphing the nrc sentiment analysis of the entire dataset
nrc_plot<-lyrics_nrc %>% group_by(sentiment) %>% summarise(word_count=n()) %>% ungroup() %>% mutate(sentiment=reorder(sentiment, word_count)) %>% ggplot(aes(sentiment,word_count)) + geom_col() + ggtitle("NRC sentiment") + coord_flip() 
nrc_plot

# It appears that NRC strongly favors positive. But the question is are all the words with sentiment of disgust/anger also in the negative category as well? It may be worth checking out. 

# Now lets take a look at bing sentiment

bing_plot<-lyrics_bing %>% group_by(sentiment) %>% summarise(word_count=n()) %>% ungroup() %>% mutate(sentiment=reorder(sentiment, word_count)) %>% ggplot(aes(sentiment,word_count)) + geom_col() + ggtitle("Bing Sentiment") + coord_flip()
bing_plot

# There appears to be more words with negative sentiment than positive sentiment. 
# so far we have been looking into single words and did sentiment analysis on single words. If the love is common words, what if there are preceding or following words like 'not' then our analysis is wrong. Looking at single words sometimes could be misleading. So now it is the time to look at bigrams or word pairs

# Tokenise using ngrams/bigram
lyrics_bigrams<-song_lyrics %>% unnest_tokens(bigram,Lyrics,token="ngrams",n=2)

lyrics_bigrams_seperated<-lyrics_bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered<-lyrics_bigrams_seperated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)

# since there are more repeated words in the song, lets do some filtering
glimpse(bigrams_filtered)
bigram_decade <- bigrams_filtered %>% filter(word1 != word2) %>% unite(bigram, word1, word2, sep=" ") %>% inner_join(song_lyrics) %>% count(bigram,decade, sort=TRUE) %>% group_by(decade) %>% slice(seq_len(7)) %>% ungroup() %>% arrange(decade,n) %>% mutate(row=row_number())

# visualize the bigram
bigram_decade %>% ggplot(aes(row, n, fill = decade)) + geom_col(show.legend = FALSE) + facet_wrap(~decade, scales = "free") + scale_x_continuous(breaks = bigram_decade$row, labels = bigram_decade$bigram) + coord_flip()

# Using bigrams, you can almost see the common phrases shift from love, dance, sex and rap style songs. 

# This time use the AFINN lexicon to perform sentiment analysis on word pairs, looking at how often sentiment-associated words are preceded by "not" or other negating words.

AFINN<-get_sentiments("afinn")

not_words<-lyrics_bigrams_seperated %>% filter(word1=="not") %>% inner_join(AFINN, by=c(word2="word")) %>% count(word2, score, sort = TRUE) %>% ungroup()

not_words %>% mutate(contribution=n*score) %>% arrange(desc(abs(contribution))) %>% head(20) %>% mutate(word2=reorder(word2,contribution)) %>% ggplot(aes(word2, n*score, fill = n*score>0)) + geom_col(show.legend = FALSE) + xlab("Words preceded by \"not\"") +   ylab("Sentiment score * Number of Occurrences") +   ggtitle("Sentiment of Words Preceded by Not") +  coord_flip()

#  Words like care, love, good is given a false positive sentiment because the "not" is ignored with single-word analysis


negation_words<-c("not","no","never","without")

negation_bigrams<-lyrics_bigrams_seperated %>% filter(word1 %in% negation_words) %>% inner_join(AFINN, by = c(word2="word")) %>%
count(word1,word2,score,sort=TRUE) %>% mutate(contribution=n*score) %>% arrange(desc(abs(contribution))) %>% group_by(word1) %>% slice(seq_len(20)) %>% arrange(word1, desc(contribution)) %>% ungroup()

negation_bigrams 

bigram_graph<-negation_bigrams %>% graph_from_data_frame()

set.seed(123)

a<-grid::arrow(type = "closed", length = unit(.15,"inches"))

ggraph(bigram_graph, layout = "fr") + geom_edge_link(alpha=.25) + geom_edge_density(aes(fill=score)) + geom_node_point(color = "purple1", size = 1) + geom_node_text(aes(label = name),  repel = TRUE) +   theme_void() + theme(legend.position = "none",plot.title = element_text(hjust = 0.5)) +   ggtitle("Negation Bigram Network")

# Here, you can see the word pairs associated with negation words. So if your analysis is based on unigrams and "alone" comes back as negative, the bigram "not alone" as you see above will have a reverse effect. Some words cross over to multiple nodes which can be seen easily in a visual like this one: for example, "never loved" and "not good".

# Lets now take a look at the pairwise correlation
# Use the pairwiser_count() function from widyr package to identify co-occurence of events. That is, we count the number of times each pair of words appear together within a song. 



pwc<-song_lyrics_filtered %>% filter(n()>=20) %>% pairwise_count(word, Song, sort=TRUE) %>% filter(item1 %in% c("love", "care", "hurt","crazy")) %>% group_by(item1) %>% slice(seq_len(7)) %>% ungroup() %>% mutate(row=-row_number())

pwc %>%
  ggplot(aes(row, n, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~item1, scales = "free") +
  scale_x_continuous( breaks = pwc$row, labels = pwc$item2) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Pairwise Counts") +
  coord_flip()
```


```{r}
# Lets now compare it to pairwise correlation

song_lyrics_filtered %>% group_by(word) %>% filter(n()>=20) %>% pairwise_cor(word, Song, sort = TRUE) %>% filter(item1 %in% c("love", "care", "hurt","crazy")) %>% group_by(item1) %>% top_n(7) %>% ungroup() %>% mutate(item2=reorder(item2, correlation)) %>% ggplot(aes(item2, correlation, fill = item1)) + geom_bar(stat = 'identity',show.legend = FALSE) + facet_wrap(~item1, scales = 'free') + xlab(NULL) + ylab(NULL) +
  ggtitle("Pairwise Correlation") +   coord_flip()

# It is interesting to see how the words are correlated differently when compared to their paired counts. Looking at the results, we can begin to see that some themes emerge just for four words. This leads us to topic modeling

```

```{r}
# Now its time to do some topic modeling. 

# Rather than using entire dataset, we will use dataset that contains lyrics from eight artists and four books. 
all_sources_tidy_balanced<-read_csv("C:/Rekha/Personal/NLP/all_sources_tidy_balanced.csv")

# The goal of topic modeling is finding significant thematically related terms(topics) in unstructured textual data measured as patterns of word co-occurence. The basic components of topic models are documents, terms and topics. LDA(Latent Dirichlet allocation) is an unsupervised learning method which discovers different topics underlying a collection of documents where each document is collection of words. LDA makes following assumptions: 1) Each document is collection of one or more topics 2)Each topic is mixture of words. LDA seeks to find group of related words. It is iterative algorithm. Here are the two main steps. 1) During initialization, each word is assigned to random topic. 2) The algorithm goes through each word and reassigns the word to a topic with the following consideration: the probability the word belongs to the topic and the probability the document will be generated by topic. The concept behind LDA is the words that belong to a topic will appear together in documents. It tries to model each document as mixture of topic and each topic as mixture of words. you can then use propability that a document belongs to topic to classify it accordingly.  

# Lets examine the data
all_sources_tidy_balanced %>% group_by(source) %>% mutate(word_count=n(),source_document_count=n_distinct(document)) %>% select(source, genre, word_count, source_document_count) %>% distinct() %>% ungroup()

# This dataset contains a wide variety of artists ranging from beatles(pop), jay-z(rap) and johnny-cash(country). These artists were chosen because they were ranked as most profilic in their respective genre. 

# lets do some quick cleaning before we proceed
all_sources_tidy_balanced<-all_sources_tidy_balanced %>% mutate(source = ifelse(source=="machine_learning","m_learn",ifelse(source=="machine_learning_r","m_learn_r",ifelse(source=="michael_jackson","mi_jackson",ifelse(source=="sports_nutrition","nutrition",source))))) %>% mutate(genre = ifelse(genre == "machine_learning", "m_learn",ifelse(genre ==  "sports_nutrition", "nutrition", genre)))

# create DTM and set the variables
all_sources_dtm_balanced<-all_sources_tidy_balanced %>% count(document,word,sort = TRUE) %>% ungroup() %>% cast_dtm(document,word,n)

source_dtm <- all_sources_dtm_balanced
source_tidy <- all_sources_tidy_balanced

# Fit the model and identify the themes
K<-8
num_words<-10
seed<-1234

lda<-LDA(source_dtm, k=8, method = "GIBBS", control = list(seed = seed))
 
lda

song_topics<-tidy(lda, matrix = "beta")
song_topics

# Note this has turned into one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. 

song_top_terms<-song_topics %>% group_by(topic) %>% top_n(10, beta) %>% ungroup() %>% arrange(topic, -beta)
song_top_terms %>% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = 'free') + coord_flip()

# we can interpret the results as follows

    #Topic One - machine learning and data science
    #Topic Two - pop/rock
    #Topic Three - sports nutrition
    # Four - country
    #Topic Five - hip-hop-rap
    #Topic Six - country or pop-rock
    #Topic Seven - icebergs, earth science
    #Topic Eight - religious

# this function can be used to show genre and source by passing type
top_items_per_topic<-function(lda_model,source_tidy,type)
{
  #get the tidy version by passing gamma for the per document per topic prob
  document_lda_gamma<-tidy(lda_model,matrix="gamma") %>%
    inner_join(source_tidy) %>%
    select(document,gamma,source,genre,topic) %>%
    distinct() %>%
    # group so that we can get sum per topic/source
    group_by(source,topic) %>% 
    arrange(desc(gamma)) %>%
    #create the sum of all document gamma vals per topic/source
    mutate(topic_sum=sum(gamma)) %>%
    select(topic,topic_sum,source,genre) %>%
    distinct() %>%
    ungroup() %>%
    # type will be either source or genre
    group_by(source, genre) %>%
    #get the highest topic_sum per type
    top_n(1, topic_sum) %>%
    mutate(row = row_number()) %>%
    mutate(label = ifelse(type == "source", source, genre),title = ifelse(type == "source", "Recommended Writers Per Topic","Genres Per Topic"))  %>%
    ungroup() %>%
    mutate(topic = paste("Topic", topic, sep = " ")) %>%
    select(label, topic, title)
    document_lda_gamma %>% ggplot(aes(1,1,label = label, fill= factor(topic))) + geom_point(color = "transparent") +
  
    geom_label_repel(nudge_x=.2,direction="y",box.padding=0.1,segment.color='transparent',size=3) + facet_grid(~topic) + 
    xlab(NULL) + ylab(NULL) +
    ggtitle(document_lda_gamma$title) + 
    coord_flip()
  
}
  
top_items_per_topic(lda, source_tidy, "source")

# We may not be familiar with these artist. so lets try replacing the artist with genre
top_items_per_topic(lda, source_tidy, "genre")

# The hip-hop-rap artists are grouped together; the Christian artists are together; two of the pop-rock artists are together, and each book has its own topic! It's pretty amazing to see that unsupervised learning can produce results like this (recognizing different genres) based entirely on text



```

